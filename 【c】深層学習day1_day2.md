***
>### [3か月で現場で潰しが効くディープラーニング講座][1]
>[1]:http://study-ai.com/jdla

>![alt](http://ai999.careers/bnr_jdla.png)
***

# E資格取得のための学習内容レポート
<br>

## 深層学習　S1　入力層～中間層
### 講義内容の要点

　　・NNでできること

　　　-回帰（結果予想、ランキング等）

　　　-分類（文字認識、画像判別等）

　　・入力層…説明変数等、外部からデータを取り込む機能。取り込んだデータはそのまま中間層へ引き渡す（重みをかけバイアス項を加算して渡す？）。

　　・中間層…各種処理（この講義内では詳細説明なし）


### 確認テスト1
　　ディープラーニングは、結局何をやろうとしているか2行以内で述べよ。<br>また、次の中のどの値の最適化が最終目的か。全て選べ。（1分）<br>①入力値[ X]<br>②出力値[ Y]<br>③重み[W]<br>④バイアス[b]<br>⑤総入力[u] <br>⑥中間層入力[ z]<br>⑦学習率[ρ]NN全体像確認テスト

　　＜回答＞<br>
　　　やろうとしていること：入力データから最適な出力となるパラメータを発見する。<br>
　　　最終目的：③重み、④バイアス
<br>
　　　
### 確認テスト2
　　次のネットワークを紙にかけ。<br>　　　入力層︓2ノード1層<br>　　　中間層︓３ノード2層<br>　　　出力層︓1ノード1層（5分）
　　＜回答＞<br>
![alt](https://user-images.githubusercontent.com/77253188/104692354-2cc1e380-574b-11eb-8ca6-057416547cee.png)

<br>

### 確認テスト3
　　（略）
<br>
<br>


## 深層学習　S2　活性化関数
### 講義内容の要点

　　・活性化関数とは

　　　次の層への出力の大きさを決定する非線形関数。

　　・中間層で使用する活性化関数

　　　-ReLU関数　…x(x>0)、0(x≦0)

　　　-シグモイド関数　（機械学習の頁参照）

　　　-ステップ関数　…　１(x≧0）、　0（x<0)）

　　・出力層の活性化関数（詳細は深層学習　S3　出力層の項参照）

　　　-ソフトマックス関数

　　　-恒等写像

　　　-シグモイド関数

　　・全結合

　　　上位層と回想の全ノード間で結合

<br>

### 確認テスト5
　　線形…変数間が線形結合

　　非線形…変数間が非線形結合

<br>

## 深層学習　S3　出力層
### 講義内容の要点

　　・役割

　　　中間層から受け取った結果を比較対象（訓練データ）と比較し中間層での処理状態を判定する。

　　　-誤算関数…訓練データと中間層からの出力値との差分の関数（種類はいくつか存在）<br>
　　　　平均二乗誤差、クロスエントロピー誤差等。

　　・活性化関数

　　　-中間層との相違点<br>
　　　　〇信号の大きさは無変換<br>
　　　　〇出力ノードの合計が必ず1とする<br>
　　　　〇中間層で記述済のように使用する活性化関数の種類が異なる<br>
　　　　〇出力層の活性化関数（

　　　　　◇ソフトマックス関数　f(i,u)=exp(u(i))/Σ(exp(u(k))

　　　　　◇恒等写像　f(u)=u

　　　　　◇シグモイド関数　f(u)=1/(1+exp(-u))




### 確認テスト5
　　・なぜ、引き算でなく二乗するか述べよ<br>・下式の1/2はどういう意味を持つか述べよ（2分）

　　＜回答＞

　　　・プラスもマイナスも同等に罰則を与えるため。<br>
　　　・微分したとき計算をしやすくするため。

































